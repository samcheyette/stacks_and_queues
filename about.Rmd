---
title: "Model fitting"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,fig.width=5, 
                      fig.height=4,fig.align = "center",cache=TRUE, message=FALSE,warning=FALSE)
```


```{r, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
##libraries, globals

library(ggplot2)
library(reshape)
library(grid)
library(dplyr)
#library(gridExtra)
library(lme4)
library(reghelper)
library(RColorBrewer)
#library(e1071)   
library(robustbase)
library(tidylog)
library(hash)



paper_theme <- theme_light() + theme( axis.title.x = element_text(size=18),
  axis.text.x=element_text(colour="#292929", 
                           size = 14), 
  axis.title.y = element_text(size = 18, vjust = 1),
  axis.text.y  = element_text(size = 14, colour="#292929"),
  strip.text=element_text(size=16,color="black"),
strip.background = element_rect(colour = "grey50", fill = "white"),
panel.background = element_rect(fill = "white", colour = "grey50"),
  axis.ticks.x=element_blank(),axis.ticks.y=element_blank(),
  axis.line.x = element_line(colour = "black"), 
  axis.line.y = element_line(colour = "black"),
  legend.title=element_text(size=18),
  legend.text=element_text(size=15),
  panel.grid.major = element_blank(), panel.grid.minor = element_blank())


quantile_low <- function(x) {
  return(quantile(x,0.25))
}

quantile_high <- function(x) {
  return(quantile(x,0.75))
}



open_brackets <- c("(","[","{","<")
closed_brackets <- c(")","]","}",">")
matching_brackets <- hash()
for (i in 1:length(open_brackets)) {
  matching_brackets[open_brackets[i]] = closed_brackets[i]
  matching_brackets[closed_brackets[i]] = open_brackets[i]
}


f_is_CE <- function(seq, l) {
  seq <- seq[1]
  l <- l[1]
  if (nchar(seq) != l) {
    return(FALSE)
  }

  half <- round(nchar(seq)/2)
  seq_1 <- substr(seq,1,half)
  seq_2 <- substr(seq,half+1,nchar(seq))
  
  if (nchar(seq_1) != nchar(seq_2)) {
    return(FALSE)
  }
  else {
    for (i in 1:nchar(seq_1)) {
      if (substr(seq_2,i,i) != matching_brackets[[substr(seq_1,nchar(seq_1)-i+1, nchar(seq_1)-i+1)]]) {
        return(FALSE)
      }

    }
    return(TRUE)
  }
}

f_is_crossed <- function(seq, l) {
  seq <- seq[1]
  l <- l[1]
  if (nchar(seq) != l) {
    return(FALSE)
  }

  half <- round(nchar(seq)/2)
  seq_1 <- substr(seq,1,half)
  seq_2 <- substr(seq,half+1,nchar(seq))
  
  if (nchar(seq_1) != nchar(seq_2)) {
    return(FALSE)
  }
  else {
    for (i in 1:nchar(seq_1)) {
      if (substr(seq_2,i,i) != matching_brackets[[substr(seq_1,i,i)]]) {
        return(FALSE)
      }

    }
    return(TRUE)
  }
}

f_is_corr <- function(cond, seq, l) {
  cond <- cond[1]
  seq <- as.character(seq[1])
  l <- l[1]
  if (cond == "CE") {
    return(f_is_CE(seq,l)) 
  } else {
    return(f_is_crossed(seq,l))
  }
}

f_factorial <- function(l) {
  l <- l[1]
  if (l <= 1) {
    return (1)
  } else {
    return(l * f_factorial(l-1))
  }
  
}

f_gompertz <- function(t, a,b,c) {
      return(a * exp(-exp(b - c * t)))
}
```

```{r}
df_param <- read.csv("model_output/MLE_param_multstrat2.csv")
df_param$id <- seq.int(1,nrow(df_param))
df_param$cond <- gsub("crossed","Crossed",df_param$cond)
df_param$param_name <- gsub("swap_noise","memory_err",df_param$param_name)
df_param$param_name <- gsub("switch_cost","switch_time",df_param$param_name)


h_q <- hash()
for (i in 1:nrow(df_param)) {
  
  if (!has.key(as.character(df_param[i,]$pid),h_q)) {
    h_q[[as.character(df_param[i,]$pid)]] <- hash()
  }
  h_q[[as.character(df_param[i,]$pid)]][as.character(df_param[i,]$param_name)] <- df_param[i,]$param_est_Q
}



df_param_sub <- df_param %>%
          group_by(pid,cond) %>%
          top_n(n=1,wt=id) %>%
          mutate(AIC_diff = -2*(lkhd_Q-lkhd_S)) %>%
          mutate(AIC_diff_4 = -2*(lkhd_Q_4-lkhd_S_4)) %>%
          mutate(AIC_diff_6 = -2*(lkhd_Q_6-lkhd_S_6)) %>%
          mutate(AIC_diff_8 = -2*(lkhd_Q_8-lkhd_S_8)) %>%
          mutate(AIC_diff_CE=(AIC_diff)*(cond=="CE")) %>%
          mutate(AIC_diff_crossed=(AIC_diff)*(cond=="Crossed")) %>%
          mutate(strat_err_Q_4_end = f_gompertz(1, h_q[[as.character(pid[1])]][["strat_a_4"]],
                                               h_q[[as.character(pid[1])]][["strat_b_4"]],  h_q[[as.character(pid[1])]][["strat_c_4"]])) %>%
          mutate(strat_err_Q_6_end = f_gompertz(1, h_q[[as.character(pid[1])]][["strat_a_6"]],
                                               h_q[[as.character(pid[1])]][["strat_b_6"]],  h_q[[as.character(pid[1])]][["strat_c_6"]])) %>%
          mutate(strat_err_Q_8_end = f_gompertz(1, h_q[[as.character(pid[1])]][["strat_a_8"]],
                                               h_q[[as.character(pid[1])]][["strat_b_8"]],  h_q[[as.character(pid[1])]][["strat_c_8"]])) %>%
          ungroup %>%
          mutate(AIC_diff_CE = sum(AIC_diff_CE)) %>%
          mutate(AIC_diff_crossed = sum(AIC_diff_crossed)) 
```



```{r, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}

df <- read.csv("model_output/MLE_fit_multstrat2.csv")
#df <- read.csv("model_output/MLE_fit_noswitch1.csv")

df$id <- seq.int(1,nrow(df))
df$cond <- gsub("crossed","Crossed",df$cond)
df$Q_rt_mu <- exp(log(df$Q_rt_mean) + (df$Q_rt_sd**2.)/2)
df$S_rt_mu <- exp(log(df$S_rt_mean) + (df$S_rt_sd**2.)/2)
#df$Q_rt_sigma <- (exp(df$Q_rt_sd**2.-1) * (exp(2*log(df$Q_rt_mean) + df$Q_rt_sd**2.)))**0.5


df <- df %>%
      mutate(p_corr=(p_CE*(cond=="CE")) + (p_crossed*(cond=="Crossed"))) %>%
      group_by(length) %>%
      mutate(n_corr = f_factorial(round(length/2))) %>%

      mutate(n_oocc = n_corr**2) %>%
      mutate(Q_p_corr=(1-strat_err_Q) + strat_err_Q*(n_corr/n_oocc)) %>%
      group_by(pid, length) %>%
      mutate(Q_p_corr_mean=mean(Q_p_corr)) %>%
      mutate(strat_err_Q_mean=mean(strat_err_Q)) %>%
      rowwise() %>%
      mutate(Q_rt_lower=exp(log(Q_rt_mean)-Q_rt_sd)) %>%
      mutate(Q_rt_upper=exp(log(Q_rt_mean)+Q_rt_sd)) %>%
      mutate(S_rt_lower=exp(log(S_rt_mean)-S_rt_sd)) %>%
      mutate(S_rt_upper=exp(log(S_rt_mean)+S_rt_sd)) %>%
      mutate(Q_memory_err=h_q[[as.character(pid)]][["memory_err"]]) %>%
      group_by(pid,trial,length) %>%
      mutate(corr=1*f_is_corr(cond,resp,length)) %>%

      mutate(gen_length=max(index)+1) %>%
      group_by(cond,length,index) %>%
      mutate(Q_rt_upper=mean(Q_rt_upper)) %>%
      mutate(Q_rt_lower=mean(Q_rt_lower)) %>%
      mutate(S_rt_upper=mean(S_rt_upper)) %>%
      mutate(S_rt_lower=mean(S_rt_lower)) 

# beta(lm(data=df,rt~Q_rt_mean))
# beta(lm(data=df,rt~S_rt_mean))
df_sub <- df %>%
          group_by(cond,pid, trial,length) %>%
          top_n(n=1,wt=id)

```



<br/> 

### <u>Learning over time</u>

We can now turn to how each model actually fits the data. The first thing to look at is the inferred correct strategy use over time. Below is the queue model's predictions:


```{r,fig.width=9,fig.height=3}
ggplot(data=df_sub, aes(x=trial, y=1-strat_err_Q,color=cond)) +
      stat_summary(fun="mean",geom="point") +
      stat_summary(fun="mean",geom="line") +
      stat_summary(fun.data="mean_cl_boot",geom="errorbar",width=0.5) +
      paper_theme + theme(legend.position=c(0.89,0.27),legend.title=element_blank()) +
      facet_wrap(~length,scales="free_x") +
      scale_color_manual(values=c("dodgerblue","orange")) +
      coord_cartesian(ylim=c(0,1)) +
      labs(x="Trial", y="P(correct strategy)") +
      ggtitle("Correct strategy use (Queue)")


```
Next, the stack model's predictions:

```{r,fig.width=9,fig.height=3}
ggplot(data=df_sub, aes(x=trial, y=1-strat_err_S,color=cond)) +
      stat_summary(fun="mean",geom="point") +
      stat_summary(fun="mean",geom="line") +
      stat_summary(fun.data="mean_cl_boot",geom="errorbar",width=0.5) +
      paper_theme + theme(legend.position=c(0.89,0.27),legend.title=element_blank()) +
      facet_wrap(~length,scales="free_x") +
      scale_color_manual(values=c("dodgerblue","orange")) +
      coord_cartesian(ylim=c(0,1)) +
      labs(x="Trial", y="P(correct strategy)") +
      ggtitle("Correct strategy use (Stack)")


```

The only notable difference is that the stack model predicts a lower rate of correct strategy use in the 4-item crossed condition. It's not entirely clear to me why this is....

<br/> 

### <u>Reaction times</u>


Looking at the predicted (mean) reaction time under the queue model, wee see that it fits both conditions pretty well, but the center-embedded condition in particular. There are decreasing RTs in the 8-item crossed condition that it does not really capture. 

<br/> 

```{r,fig.width=9,fig.height=4}

ggplot(data=df) +
     # geom_ribbon(aes(x=index, y=Q_rt_mean,ymin=Q_rt_lower,ymax=Q_rt_upper),alpha=0.12) +
    #  stat_summary(aes(x=index,y=Q_rt_mean), geom="line",fun="mean",size=1.5) +
      stat_summary(aes(x=index,y=Q_rt_mu), geom="line",fun="mean",size=1.5) +      stat_summary(aes(x=index,y=rt,color=cond),fun="mean") +
      stat_summary(aes(x=index,y=rt,color=cond),fun.data="mean_cl_boot",geom="errorbar",width=0.2) +
      facet_grid(cond~length,scales="free_x") +
            scale_color_manual(values=c("dodgerblue","orange")) +
      paper_theme + guides(color="none") +
      labs(x="Index",y="RT") +
      ggtitle("Queue vs. Human RT")

```
<br/> 

The stack model misses the key trend of decreasing RTs in the second half of 6-item and 8-item lists in the center-embedded condition. It slightly better captures the longer RT for the first item in the second half of the list in the crossed condition than the queue model.

<br/> 

```{r,fig.width=9,fig.height=4}

ggplot(data=df) +
   #  geom_ribbon(aes(x=index, y=S_rt_mean,ymin=S_rt_lower,ymax=S_rt_upper),alpha=0.12) +
     # stat_summary(aes(x=index,y=S_rt_mean), geom="line",fun="mean",size=1.5) +
      stat_summary(aes(x=index,y=S_rt_mu), geom="line",fun="mean",size=1.5) +
      stat_summary(aes(x=index,y=rt,color=cond),fun="mean") +
      stat_summary(aes(x=index,y=rt,color=cond),fun.data="mean_cl_boot",geom="errorbar",width=0.2) +
      facet_grid(cond~length,scales="free_x") +
            scale_color_manual(values=c("dodgerblue","orange")) +
      paper_theme + guides(color="none") +
      labs(x="Index",y="RT") +
      ggtitle("Stack vs. Human RT")

```

<br/> 

### <u>Accuracy</u>

Below is a plot of people's response accuracy in both conditions. There are two things to note: 1) people are more accurate in the crossed-serial condition; 2) in both conditions, their accuracy decreases with sequence length. Compare the below plot to the models' predicted accuracies, shown on the previous page (the final plot). (Hint: it looks much more queue-like than stack-like.)

<br/> 

```{r,fig.width=5,fig.height=3.5}

ggplot(data=df) +
      stat_summary(aes(x=length, y=corr, color=cond),fun="mean",geom="line",size=0.8) +
      stat_summary(aes(x=length, y=corr, color=cond),fun.data="mean_cl_boot",geom="errorbar",width=0.2,size=0.8) +

      stat_summary(aes(x=length, y=corr, color=cond),fun="mean",geom="point",size=2.75) +
      stat_summary(aes(x=length, y=corr,group=cond),fun="mean",geom="point",size=1.75,color="white") +
      scale_color_manual(values=c("dodgerblue","orange")) +
      labs(x="Length",y="P(correct)") +
      paper_theme + theme(legend.title=element_blank(), legend.position=c(0.84,0.84))
      


```

<br/> 

We can look at what the model says about the cause of people's errors. Note that I'm just showing predictions from the queue model below. First, we can look at the correlation between participants' inferred memory noise and their accuracy in both conditions and for each sequence length. There's obviously a strong relationship, but it's strongest in the center-embedded condition for longer sequences. 

```{r,fig.width=8,fig.height=4}


ggplot(data=df) +
        stat_summary(aes(x=Q_memory_err, y=corr,group=pid,color=cond),fun="mean",geom="point") +
        geom_smooth(aes(x=Q_memory_err, y=corr), method="lm",formula=y~x,se=FALSE,color="black") +
      paper_theme + guides(color="none") +
        facet_grid(cond~length) +
      labs(x="Memory noise") + ylab("P(correct)") +
        scale_x_continuous(breaks=c(0.0,0.05,0.10)) +
            scale_color_manual(values=c("dodgerblue","orange")) 
```

<br/> 

We can next turn to inferred strategy errors' relationship with accuracy, shown below. The correlations here are even stronger than with memory errors across the board, and are strongest in both conditions for longer sequences. This tells us that a significant fraction of people's incorrect responses were from them not using the correct strategy, rather than incorrectly remembering the order they pressed items or from inattention.

```{r,fig.width=8,fig.height=4}

ggplot(data=df) +
        stat_summary(aes(x=strat_err_Q_mean, y=corr,group=pid,color=cond),fun="mean",geom="point") +
        geom_smooth(aes(x=strat_err_Q_mean, y=corr), method="lm",formula=y~x,se=FALSE,color="black") +
      paper_theme + guides(color="none") +
        facet_grid(cond~length) +
      labs(x="Strategy error") + ylab("P(correct)") +
            scale_color_manual(values=c("dodgerblue","orange")) +
      scale_x_continuous(breaks=c(0,0.25,0.5,0.75,1.), labels=c("0.0","0.25","0.5","0.75","1.0"))



```

<br/>

### <u>Goodness of fit</u>

Finally, we can look at the AIC of the stack versus queue models. The overall AIC difference is 1467 in favor of the queue model over the stack model. However, the evidence in favor of queues only comes from the center-embedded condition. In fact, stacks fit the crossed-serial data slightly better.

<br/> 

```{r,fig.width=7,fig.height=3}


ggplot(data=df_param_sub) +
      geom_hline(yintercept=0,linetype="dotted") +

      stat_summary( aes(x="4",y=AIC_diff_4),height=0,width=0.35,color="red",geom="bar",fun="mean") +
      geom_jitter( aes(x="4",y=AIC_diff_4),height=0,width=0.05,alpha=0.5) +
      stat_summary( aes(x="4",y=AIC_diff_4),color="red",geom="point",fun="mean",alpha=0.8) +
      stat_summary( aes(x="4",y=AIC_diff_4),height=0,width=0.05,color="red",geom="errorbar",fun.data="mean_cl_boot",alpha=0.8) +
        stat_summary( aes(x="6",y=AIC_diff_6),height=0,width=0.35,color="red",geom="bar",fun="mean") +
      geom_jitter( aes(x="6",y=AIC_diff_6),height=0,width=0.05,alpha=0.5) +
      stat_summary( aes(x="6",y=AIC_diff_6),color="red",geom="point",fun="mean",alpha=0.8) +
      stat_summary( aes(x="6",y=AIC_diff_6),height=0,width=0.05,color="red",geom="errorbar",fun.data="mean_cl_boot",alpha=0.8) +

  
          stat_summary( aes(x="8",y=AIC_diff_8),height=0,width=0.35,color="red",geom="bar",fun="mean") +
      geom_jitter( aes(x="8",y=AIC_diff_8),height=0,width=0.05,alpha=0.5) +
      stat_summary( aes(x="8",y=AIC_diff_8),color="red",geom="point",fun="mean",alpha=0.8) +
      stat_summary( aes(x="8",y=AIC_diff_8),height=0,width=0.05,color="red",geom="errorbar",fun.data="mean_cl_boot",alpha=0.8) +

      paper_theme + 
      labs(x="Length",y=expression(AIC[Q] - AIC[S])) +
      facet_wrap(~cond)



```
<br/> 

Looking at the average weight of evidence per participant as a function of length, collapsing across condition, we find that the 4-item case gives no indication of whether stacks or queues fit better. The 6- and 8-item data show a strong trend towards queues, however.
<br/> 

```{r,fig.width=5,fig.height=3.5}


ggplot(data=df_param_sub) +
      geom_hline(linetype="dotted", yintercept=0)+
      stat_summary(aes(x="4",y=AIC_diff_4),fun="mean",geom="bar",width=0.5,color="black",fill="gray") +
      stat_summary(aes(x="4",y=AIC_diff_4),fun.data="mean_cl_boot",geom="errorbar",width=0.1) +
      stat_summary(aes(x="4",y=AIC_diff_4),fun="mean",geom="point",size=2) +
      stat_summary(aes(x="4",y=AIC_diff_4),fun="mean",geom="point",size=1.5,color="white") +
      stat_summary(aes(x="6",y=AIC_diff_6),fun="mean",geom="bar",width=0.5,color="black",fill="gray") +
      stat_summary(aes(x="6",y=AIC_diff_6),fun.data="mean_cl_boot",geom="errorbar",width=0.1) +
      stat_summary(aes(x="6",y=AIC_diff_6),fun="mean",geom="point",size=2) +
      stat_summary(aes(x="6",y=AIC_diff_6),fun="mean",geom="point",size=1.5,color="white") +
      stat_summary(aes(x="8",y=AIC_diff_8),fun="mean",geom="bar",width=0.5,color="black",fill="gray") +
      stat_summary(aes(x="8",y=AIC_diff_8),fun.data="mean_cl_boot",geom="errorbar",width=0.1) +
      stat_summary(aes(x="8",y=AIC_diff_8),fun="mean",geom="point",size=2) +
      stat_summary(aes(x="8",y=AIC_diff_8),fun="mean",geom="point",size=1.5,color="white") +
      # geom_jitter(aes(x="4", y=AIC_diff_4),alpha=0.5,size=0.8,height=0,width=0.1) +
      # geom_jitter(aes(x="6", y=AIC_diff_6),alpha=0.5,size=0.8,height=0,width=0.1) +
      # geom_jitter(aes(x="8", y=AIC_diff_8),alpha=0.5,size=0.8,height=0,width=0.1) +

      paper_theme +
      labs(x="Length",y=expression(AIC[Q] - AIC[S])) 


```
